{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "748800df",
   "metadata": {},
   "source": [
    "# Continuous Control Report\n",
    "\n",
    "To complete this project, I went on a terrible dark journey of despair and hope, and ultimately came out the other side with a working implementation and a bit of experience.  My journey consisted of 3 main parts:\n",
    "  - Initially I was drawn to the description of the Proximal Policy Optimization (PPO) algorithm, after [reading](https://openai.com/blog/openai-baselines-ppo/) that it is simple and often more stable than other algorithms. I was greatly inspired by the beautiful modular implementation demonstrated in the lessons at [S. Zhang's](https://github.com/ShangtongZhang/DeepRL), and tried to adapt his PPO continuous module to my uses, trying both the single and multiple agent environments.  This was a spectacular failure that cost me more than a week of trouble.  I was unable to demonstrate any learning, and the small initial scores would immediately decay after more training.  Perhaps some of the lessons learned later can be applicable to this situation, but I tried changing all conceivable parameters without seeing any improvement.\n",
    "  \n",
    "![image](./images/dark_woods.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe2e2f9",
   "metadata": {},
   "source": [
    "  - Out of frustration more than anything, I decided to try using the Deep Deterministic Policy Gradient (DDPG) code as described in the project instructions.  There is a DDPG agent code described in the \\deep-reinforcement-learning\\ddpg-pendulum code from our lessons.  With essentially 0 changes to this code, it was able to demonstrate at least some learning before collapsing ![image](./images/result0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a68345",
   "metadata": {},
   "source": [
    "## Batch Size Exploration\n",
    "I started playing with the batch_size, and observed that the learning could stay more constant with smaller batches, albeit with a poor level of learning.  \n",
    "### Batch size 32 from 128\n",
    "Takes longer to start learning, but then doesn't collapse\n",
    "![image](./images/result1.png)\n",
    "\n",
    "### Batch size 8 \n",
    "Worse learning, but does not collapse.\n",
    "![image](./images/result2.png)\n",
    "\n",
    "I additionally explored the gradient clip level, without any effect.\n",
    "\n",
    "## Learning Rate Exploration\n",
    "The initial learning rates were 1e-3 for both the actor and critic.  Reducing these to 1e-4 gave much better learning.  The batch size here was back at 128\n",
    "\n",
    "![image](./images/result3.png)\n",
    "\n",
    "The batch size was reduced in steps of /2 until at a size of 8 it finally gave a good result, solving the environment in 500 episodes.\n",
    "\n",
    "![image](./images/result4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda3a595",
   "metadata": {},
   "source": [
    "## Code Cleanup\n",
    "  - With an okay result in hand, in the final step of my journey I ported the code to the modular framework that I had been using, the one inspired by Zhang's repo.  The same sort of result was obtained as previously, but using this nice flexible style that I hope will allow me to explore additional algorithms and problems at work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a3343c",
   "metadata": {},
   "source": [
    "## Lessons\n",
    "  - ### Start Small\n",
    "    - It's quite difficult to try to start with a full-fledged implementation like Zhang's, that has lots of moving parts that you may not quite understand.  It's more fruitful to test each piece with a Jupyter notebook and then transfer to a python file.\n",
    "  - ### Hyperparameters are Super Important\n",
    "    - With other machine learning projects, it has been fairly easy to demonstrate at least some learning.  When you get something to work a little bit, it's so much easier to tune things than when there is no learning at all, like my PPO implementation.\n",
    "    - The speed of the learning has a large effect on stability.  In my case the learning rate and batch size were the most important.  I had not expected batch size to play a big role.  It appeared that using a larger batch size kept the agent from finding the best policy direction.  Perhaps it's better to pick a certain direction and follow it than follow an averaged out direction that may lead to a bad spot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864208b1",
   "metadata": {},
   "source": [
    "## Directions for Improvement\n",
    "There are numerous directions for improvement, and my DDPG agent is very basic.\n",
    "  - ### Exploration Noise\n",
    "    - I did not add any exploration noise.  This might have a big effect on the ability of the agent to find a good policy\n",
    "  - ### Batch Normalization\n",
    "    - The DDPG paper notes that normalizing the input states by subtracting the means and dividing by standard deviations can be important.\n",
    "  - ### Distributed Learning\n",
    "    - I would want to get the multi-environment version of the algorithm to work, to speed up learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c8a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
